## 차원축소(Dimension Reduction)

### 차원축소의 이해

#### 차원의 저주

 [차원의 저주 참조](https://bioinformaticsandme.tistory.com/197)

[고급 선형대수](https://datascienceschool.net/02%20mathematics/03.00%203%EC%9E%A5%20%EA%B3%A0%EA%B8%89%20%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98.html)

| ![1차원](https://user-images.githubusercontent.com/70785000/125038926-eb463b80-e0d0-11eb-8291-555ec2166ac6.PNG) | ![2차원](https://user-images.githubusercontent.com/70785000/125039037-0add6400-e0d1-11eb-9682-ce89f237097b.PNG) | ![다차원](https://user-images.githubusercontent.com/70785000/125039067-129d0880-e0d1-11eb-98d9-5e548e7b1b27.PNG) |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `1차원`                                                      | `특정면적공간(2차원)`                                        | `특정입체 공간(다차원)`                                      |

차원이 좌측에서 우측으로 커질수록 

* 데이터 포인트들간 거리가 크게 늘어남
* 데이터가 희소화(`sparse`)됨
* 수백 ~ 수천개 이상의 피처로 구성된 포인트들간 거리에 기반한 ML알고리즘이 무력화됨
* 또한 피처가 많을 경우 개별 피처간에 상관관계가 높아 선형회귀와 같은 모델에서는 다중 공선성 문제로 모델의 예측 성능이 저하될 가능성이 높음



#### 차원축소의 장점

<u>수십 ~ 수백의 피처들을 작은 수의 피처들로 축소한다면?</u>

* 학습 데이터 크기를 줄여서 학습시간 절약
* 불필요한 피처들을 줄여서 모델 성능 향상에 기여할수도 있음(주로, 이미지 관련 데이터)
* 다차원의 데이터를 3차원 이하의 차원 축소를 통해서 시각적으로 보다 쉽게 데이터 패턴 인지

<span style="color:red">**그렇다면 어떻게 원본 데이터의 정보를 최대한으로 유지하면서 차원 축소를 수행할것인가?**</span>

####  피처 선택과 피처 추출

일반적으로 차원 축소는 `피처 선택(feature selection)과 피처 추출(feature extraction)`로 나눌 수 있습니다.

| `피처 선택(Feature Selection)`  | 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것(우등생만 선택) | ![feature_selection](https://user-images.githubusercontent.com/70785000/125044413-e6848600-e0d6-11eb-9499-5c6724e1abdd.PNG) |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `피처 추출(Feature Extraction)` | 피처(특성) 추출은 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것입니다. 이렇게 새롭게 추출된 중요 특성은 기존의 피처를 반영해 압축된 것이지만 새로운 피처로 추출하는 것 | ![feature_extraction](https://user-images.githubusercontent.com/70785000/125045752-4d566f00-e0d8-11eb-9bc2-49a2122714b3.PNG) |



##### 피처추출(Feature Extraction)

피처 추출은 기존 피처를 단순 압축이 아닌 , 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것(피처안에 잠재되어 있는 인자를 추출 - `latent factor`)

<u>그럼, 종합적인 학생의 능력치를 평가하고자 한다면?</u>

![feature_extraction_1](https://user-images.githubusercontent.com/70785000/125047476-f9e52080-e0d9-11eb-873a-6836017fa496.PNG)

위와 같이 `학업성취도, 커뮤니케이션, 문제해결력`과 같은 추가 피처를 도출해 볼 수 있다.

<span style="color:red">**나도 모르는 내 안의 또 다른 자아의 성격을 추출해 내는것**</span>



##### 차원 축소의 의미

차원 축소는 단순히 데이터의 압축을 의미하는 것이 아닙니다. 더 중요한 의미는 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적 요소를 추출하는데 있다.

* 추천엔진
* 이미지 분류 및 변환
* 문서 토픽 모델링

### PCA(Principle Component Analysis)의 이해

* 고차원의 원본 데이터를 저 차원으로 부분공간으로 투영하여 데이터를 축소하는 기법. 예를 들어 10차원의 데이터를 2차원의 부분 공간으로 투영하여 데이터를 축소

* PCA는 원본 데이터가 가지는 데이터 변동성을 가장 중요한 정보로 간주하여 이 변동성에 기반한 원본 데이터 투영으로 차원 축소를 수행

  | ![pca1](https://user-images.githubusercontent.com/70785000/125156445-60cd0d00-e1a0-11eb-9766-3f3cfe015ca2.PNG) | PCA는 원본 데이터 변동성이 가장 큰 방향으로 순차적으로<br />축들을 생성하고 , 이렇게 생성된 축으로 데이터를 투영하는 방식입니다. |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |

  [`A`] 데이터 변동성이 가장 큰 방향으로 축 생성

  ![pca2](https://user-images.githubusercontent.com/70785000/125156535-ddf88200-e1a0-11eb-9f71-24a5461eba4a.PNG)

​      [`B`]새로운 축으로 각각의 데이터를 투영 및 매핑

![pca3](https://user-images.githubusercontent.com/70785000/125156590-39c30b00-e1a1-11eb-9f34-3547bfe12eaa.PNG)

​    [`C`]새로운 축 기준으로 데이터 표현

![pca4](https://user-images.githubusercontent.com/70785000/125156727-ee5d2c80-e1a1-11eb-9f3b-7d2bd72b387c.PNG)

`PCA`는 제일 먼저 원본 데이터에 가장 큰 데이터 변동성(`Variance`)을 기반으로 첫 번째 벡터 축을 생성하고, 두 번째 축은 첫번째 축을 제외하고 그 다음으로 변동성이 큰 축을 설정하는 데 이는 첫번째 축에 직각이 되는 벡터(직교 벡터)축 입니다.

세번째 축은 다시 두번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성합니다. 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터 축의 갯수만큼의 차원으로 원본 데이터가 차원 축소됩니다.

| ![pca5](https://user-images.githubusercontent.com/70785000/125157402-c1127d80-e1a5-11eb-973a-3351fb2262c5.PNG) | ![pca6](https://user-images.githubusercontent.com/70785000/125168445-972a7c80-e1e0-11eb-985f-3f8a84e61df9.PNG) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

PCA, 즉 주성분 분석은 이처럼 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법.

#### PCA변환

`PCA`를 선형대수 관점에서 해석해보면, 입력 데이터의 공분산 행렬(`Covariance Matrix`)를 고유값 분해하고, 이렇게 구한 고유 벡터에 입력 데이터를 선형변환하는 것.

![PCA_trm](https://user-images.githubusercontent.com/70785000/125168881-9f83b700-e1e2-11eb-87b1-a2e6f82276b5.PNG)

* **고유벡터는 `PCA의 주성분 벡터`로서 입력 데이터의 분산이 큰 방향을 나타냅니다.**
* **`고윳값(eigentvalue)`은 바로 이 고유벡터의 크기를 나타내며, 동시에 입력 데이터의 분산을 나타냅니다.**

#### 공분산 행렬

보통 분산은 한 개의 특정한 변수의 데이터 변동을 의미하나, 공분산은 두 변수간의 변동을 의미합니다. 

즉, 사람 키 변수를 X, 몸무게 변수를 Y라고 하면 공분산 Cov(X, Y) > 0은 X(키)가 증가할때 Y(몸무게)도 증가한다는 의미입니다.

|      | X       | Y       | Z        |
| ---- | ------- | ------- | -------- |
| X    | **3.0** | -0.71   | -0.24    |
| Y    | -0.71   | **4.5** | 0.28     |
| Z    | -0.24   | 0.28    | **0.91** |

<span style="color:red">**공분산 행렬은 여러 변수와 관련된 공분산을 포함하는 정방형 행렬이며 대칭행렬입니다.**</span>

정방행렬은 열과 행이 같은 행렬을 지칭하는데, 정방행렬 중에서 대각 원소를 중심으로 원소 값이 대칭되는 행렬, 즉 A^T = A인 행렬을 대칭행렬이라고 부릅니다.

대칭행렬은 고유값 분해와 관련해 매우 좋은 특성이 있습니다. 대칭행렬은 항상 고유 벡터를 직교행렬(`orthogonal matrix`)로 , 고유값을 정방 행렬로 대각화할 수 있다는 것입니다.

#### 선형변환과 고유 벡터/ 고유값

* 일반적으로 선형변환은 특정 벡터에 행렬 A를 곱해 새로운 벡터로 변환하는 것을 의미합니다. 이를 특정 벡터를 하나의 공간에서 다른 공간으로 투영하는 개념으로도 볼 수 있으며, 이 경우 이 행렬을 바로 공간으로 가정하는 것입니다.
* 고유 벡터는 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터를 지칭합니다. 즉, Ax = ax(A는 행렬, x는 고유벡터, a는 스칼라값)입니다. 이 고유벡터는 여러 개가 존재하며, 정방 행렬은 최대 그 차원 수 만큼의 고유 벡터를 가질 수 있습니다. 예를 들어 2x2 행렬은 두 개의 고유벡터를 , 3 x 3행렬은 3개의 고유벡터를 가질 수 있습니다. 이렇게 고유벡터는 행렬이 작용하는 힘의 방향과 관계가 있어서 행렬을 분해하는 데 사용됩니다.

![pca7](https://user-images.githubusercontent.com/70785000/125201494-66af1500-e2aa-11eb-9665-f0f75d2d6c03.PNG)

#### 공분산 행렬의 고유값 분해

![pca8](https://user-images.githubusercontent.com/70785000/125201814-a5919a80-e2ab-11eb-9bba-6cdb56720f55.PNG)

* P는 n * n의 직교행렬이며 , $$ \sigma $$는 n * n 정방행렬, P^T는 행렬 P의 전치행렬
* 공분산 C는 고유벡터 직교 행렬, 고유값 정방 행렬 * 고유벡터 직교 행렬의 전치 행렬로 분해됩니다.
* e_i는 i번째 고유벡터를 , $$ \lambda $$_i는 i번째 고유벡터의 크기를 의미.<span style="color:red">고유벡터는 바로 PCA의 축입니다.</span>
* e_1는 가장 분산이 큰 방향을 가진 고유벡터이며, e_2는 e_1에 수직이면서 다음으로 가장 분산이 큰 방향을 가진 고유벡터입니다.

#### PCA변환과 수행 절차

##### PCA변환

입력데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며, 이렇게 분해된 고유벡터를 이용해 입력데이터를 선형 변환하는 방식

##### PCA변환 수행 절차

1. 입력 데이터 세트의 공분산 행렬을 생성
2. 공분산 행렬의 고유벡터와 고윳값을 계산
3. 고유값이 가장 큰 순으로 K개(PCA변환 차수만큼) 만큼 고유벡터를 추출
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환

### 사이킷런 PCA

사이킷런은 `PCA`를 위해 `PCA클래스`를 제공(`fit_transform`)

`sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)`

* `n_components`는 `PCA축의 갯수 즉 변환 차원`을 의미
* `PCA`를 적용하기 전에 입력 데이터의 개별 피처들을 스케일링 해야 합니다. `PCA`는 여러 피처들의 값을 연산해야 하므로 피처들의 스케일에 영향을 받습니다. 따라서 여러 속성을 `PCA`로 압축하기 전에 각 피처들의 값을 동일한 스케일로 변환하는 것이 필요합니다. 일반적으로 평균이 0, 분산이 1인 표준 정규분포로 변환합니다(`StandardScaler`)
* `PCA변환`이 완료된 `사이킷런 PCA객체`는 전체 변동성에서 `개별 PCA컴포넌트`별로 차지하는 변동성 비율을 `explained_variace_ratio속성`으로 제공

### LDA(Linear Discriminant Analysis)

[NIRPY Research](https://nirpyresearch.com/classification-nir-spectra-linear-discriminant-analysis-python/)

* `LDA는 선형 판별 분석법으로 불리며, PCA와 매우 유사`하다.

* `LDA`는 `PCA`와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만, 중요한 차이는 `LDA`는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소한다.

  `PCA`는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, `LDA`는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾습니다.

* `LDA`는 같은 클래스의 데이터는 최대한 근접해서 , 다른 클래스의 데이터는 최대한 떨어뜨리는 축 매핑을 합니다.

![Classification of NIR spectra by Linear Discriminant Analysis in Python](https://nirpyresearch.com/wp-content/uploads/2018/11/PCAvsLDA-1024x467.png)

#### LDA차원 축소 방식

[선형판별 분석법 LDA참조](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=mang_goo83&logNo=20095127760)

`LDA`는 특정 공간 상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산(`between-class scatter`)과 클래스 내부 분산(`within-class scatter`)의 비율을 최대화하는 방식으로 차원을 축소합니다.

즉 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부 분산은 최대한 작게 가져가는 방식입니다.

![img](https://mblogthumb-phinf.pstatic.net/20091208_193/mang_goo83_1260259231038NUwrO_png/%C1%A6%B8%F1_%BE%F8%C0%BD_mang_goo83.png?type=w210)

위 사진에서 보듯이 클래스간 분산은 최대한 많을수록 클래스간 분별이 용이하고, 클래스 내의 분산은 최대한 작게 가져가는 것이 판별하기 용이하다.(클래스 내부의 분산은 응집력이 좋도록)

#### LDA절차

일반적으로 `LDA`를 구하는 스탭은 `PCA`와 유사하나 가장 큰 차이점은 공분산 행렬이 아니라 앞에서 설명한 클래스간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영한다는 점

1. 클래스 내부와 클래스 간 분산 행렬을 구합니다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 벡터(`mean vector`)를 기반으로 구합니다.

2. 클래스 내부 분산 행렬을 이렇게 정의하고,
   $$
   S_W
   $$
   클래스 간 분산 행렬을 
   $$
   S_B
   $$
   라고 하면, 다음 식으로 두 행렬을 고유벡터로 분해할 수 있다.

![lda_cal](https://user-images.githubusercontent.com/70785000/125249243-f8149a80-e32f-11eb-8516-481b881467d8.PNG)

3. 고유값이 가장 큰 순으로 `K개(LDA변환 차수만큼) 추출`합니다.
4. 고유값이 가장 큰 수능로 `K개(LDA변환 차수만큼) 추출`합니다. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다.

### SVD(Singular Value Decomposition)

SVD역시 PCA와 유사한 행렬 분해 기법을 이용.PCA의 경우, 정방행렬(즉, 행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있지만, SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용가능.

[SVD Article](https://www.mdpi.com/1996-1073/14/8/2284/htm)



###### 고유값 분해

[고유값 분해 참조](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-19-%ED%96%89%EB%A0%AC)



###### 특이값 분해

[특이값 분해 참조](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-20-%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4Singular-Value-Decomposition)

SVD는 특이값 분해로 불리며, 행렬 U와 V에 속한 벡터는 특이벡터(singluar vector)이며 모든 특이벡터는 서로 직교하는 성질을 가집니다.

$$\sum\ $$는 대각행렬이며, 행렬의 대각에 위치한 값만이 0이 아니고, 나머지 위치의 값은 모두 0.

$$\sum\$$이 위치한 0이 아닌 값이 바로 행렬 A의 특이값

![SVD](https://user-images.githubusercontent.com/70785000/125402595-8d2a9880-e3ef-11eb-89f1-4fcb7f50148a.PNG)

![https://www.mdpi.com/energies/energies-14-02284/article_deploy/html/images/energies-14-02284-g002.png](https://www.mdpi.com/energies/energies-14-02284/article_deploy/html/images/energies-14-02284-g002.png)
$$
A = U\sum\ V^T
$$
하지만 일반적으로는 아래와 같이 $$\sum\$$의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된 $$\sum$$에 대응되는 U와 V도 원소도 함게 제거해 차원을 줄인 형태로 SVD를 적용

###### SVD유형

SVD유형에는 Full SVD, Compact SVD, Truncated SVD등이 있다.



###### Truncated SVD 행렬 분해 의미

* SVD는 차원 축소를 위한 행렬 분해를 통해 latent factor(잠재요인)를 찾을 수 있는데 이렇게 찾아진 Latent Factor는 많은 분야에 활용(추천엔진, 문서의 잠재 의미 분석등)
* SVD로 차원 축소 행렬 분해된 후 다시 분해된 행렬을 이용하여 원복된 데이터셋은 잡음(Noise)이 제거된 형태로 재구성 될 수 있음
* 사이킷런에서는 Truncated SVD로 차원을 축소할 때 원본 데이터에 U$$sigma$$를 적용하여 차원 축소
* 대각 원소 가운데 상위 r개만 추출하여 차원을 축소

![truncatedSVD](https://user-images.githubusercontent.com/70785000/125402217-1e4d3f80-e3ef-11eb-9627-d4298974ede6.PNG)

###### SVD활용

* 이미지 압축/변환
* 추천엔진
* 문서 잠재 의미 분석
* 의사(peusdo) 역행렬을 통한 모델 예측

### NMF(Non-Negative Matrix Factorization)

#### NMF개요

NMF는 Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사(Low-Rank Approximation)방식의 변형.

NMF는 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 다음과 같이 좀 더 같단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법

![nmf](https://user-images.githubusercontent.com/70785000/125416054-5efaafc6-da8f-4355-9c9a-21773e295b48.PNG)

#### 행렬분해(Matrix Factorization)

행렬분해는 일반적으로 SVD와같은 행렬 분해 기법을 통칭하는 것입니다. 이처럼 행렬 분해를 하게되면 W행렬과 H행렬은 일반적으로 길고 가는 행렬 W(즉, 원본 행렬의 행 크기와 같고 열 크기보다 작은 행렬)와 작고 넓은 행렬 H(원본 행렬의 행 크기보다 작고 열 크기와 같은 행렬)로 분해됩니다. 

이렇게 분해된 행렬은 Latent Factor(잠재 요소)를 특성으로 가지게 됩니다.

분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응하며, 분해 행렬 H는 이 잠재 요소가 원본 열(즉, 원본 속성)로 어떻게 구성됐는지를 나타내는 행렬입니다.

![nmf](https://user-images.githubusercontent.com/70785000/125416054-5efaafc6-da8f-4355-9c9a-21773e295b48.PNG)
