### 회귀 알고리즘

회귀는 현대 통계학을 이루는 큰 축이며 유전적 특성을 연구한 영국의 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다는 것이 일반적이다.

데이터분석가들은 분류보다는 회귀를 지도 학습 분야에서 가장 많이 활용한다.

"부모의 키가 크더라도 자식의 키가 대를 이어 무한정  커지지 않으며 , 부모의 키가 작더라도 대를 이어 자식의 키가 무한정 작아지지 않는다"

#### 회귀 개요

* 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭하며, 결정값이 주어지는 지도학습이라고 할 수 있다.

회귀분석은 이처럼 데이터 값이 평균과 같은 일정값으로 돌아가려는 경향을 이용한 통계학 기법

아파트 가격의 결정은 방 개수, 아파트 크기, 주변 학군, 근처 지하철 역 갯수등 다양한 독립변수로의 결합으로 이루어진다고 가정해보자.

그럼, 아래와 같은 수식을 생각해본다면,
$$
Y = W_1 * X_1 + W_2 * X_2 + W_3 * X_3 + .... + W_n * X_n
$$
`Y는 종속변수, 즉 아파트 가격`
$$
X_1, X_2, X_3, ....X_n 은 방 개수, 아파트 크기, 주변 학군등의 독립변수
$$

$$
W_1, W_2, W_3, ....W_n은 이 독립변수의 값에 영향을 미치는 회귀계수
$$

<span style="color:red"><b>`머신러닝 회귀 예측`의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것</b></span>

#### 회귀의 유형

* 회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러가지 유형으로 나눌 수 있습니다. 회귀에서 가장 중요한 것은 바로 회귀계수(`coefficient`)입니다. 이 회귀계수가 `선형이냐 비선형`에 따라 선형 회귀와 비선형 회귀로 나눌 수 있습니다. 그리고 독립 변수의 갯수가 한 개인지 여러 개인지에 따라 단일회귀, 다중 회귀로 나뉩니다.

  | 독립변수 갯수      | 회귀 계수의 결합    |
  | ------------------ | ------------------- |
  | 1개 : 단일 회귀    | 선형 : 선형 회귀    |
  | 여러 개: 다중 회귀 | 비선형: 비선형 회귀 |


#### 분류와 회귀

| 분류(Classification)                                         | 회귀(Regression)                             |
| ------------------------------------------------------------ | -------------------------------------------- |
| 결과값 : category값(이산값)으로 0, 1, 2,3과 같이 불연속적인 값<br />예) 등급, 고양이 혹은 개 판정 | 숫자값으로 연속적인 값<br />예) 비율, 점유울 |

#### 선형회귀의 종류

* 일반 선형 회귀 : 예측값과 실제 값의 `RSS(Residual Sum of Squares)`를 최소화 할 수 있도록 회귀 계수를 최적화하며, 규제(Regulation)를 적용하지 않은 모델

* `릿지(Ridge)`: 릿지 회귀는 선형 회귀에 `L2규제`를 추가한 회귀 모델

* `라쏘(Lasso)` : 라쏘 회귀는 선형 회귀에 `L1규제`를 적용한 방식

* `엘라스틱넷(ElasticNet)` : `L2, L1규제`를 함께 결합한 모델

* `로지스틱 회귀(Logistic Regression)` : 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형모델

  예) `0, 1`의 값을 예측(이산) - 바이너리 분류에 효과적

  

#### 단순 선형 회귀(Simple Regression)를 통한 회귀의 이해

주택 가격이 단순히 주택의 크기로만 결정되는 단순 선형 회귀로 가정하면 다음과 같이 주택가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현 할 수 있다.

회귀는 우측의 이미지처럼 `실제값과 모델 사이의 오류값의 차이를 최소화 하는 것`을 목표로 하며, 점선형태의  `빨간색 ` 회귀선처럼 오류차이가 많이 나면 좋지 않다고 할 수 있다.

| <img src="https://user-images.githubusercontent.com/70785000/122632039-07c30980-d10b-11eb-950c-e0106cba710a.PNG" alt="regression_1" style="zoom:80%;" /> | <img src="https://user-images.githubusercontent.com/70785000/122632286-d0edf300-d10c-11eb-853d-5805356da673.PNG" alt="regression_2" style="zoom:80%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

<span style="color:red">`최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 잔차(오류 값)합이 최소가 되는 모델을 만든다는 의미이며, 동시에 오류 값이 최소가 될 수 있는 최적의 회귀계수를 찾는다는 의미도 된다.`</span>

#### RSS기반의 회귀 오류 측정

RSS란?

오류 값의 제곱을 구해서 더하는 방식. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS방식으로 오류 합을 구합니다. 
$$
Error_2 = RSS
$$
![rss](https://user-images.githubusercontent.com/70785000/122643155-3233b780-d149-11eb-9f75-f6f70c286b62.PNG)

#### RSS의 이해

- RSS는 이제 변수가 w0,w1인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심사항

- RSS는 <span style="color:red">회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라, w변수(회귀계수)가 중심 변수 임을 인지하는 것이 매우 중요합니다.</span>(학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주합니다)

- 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.(i는 1부터 학습 데이터의 총 건수 N까지)

  ![rss_math](https://user-images.githubusercontent.com/70785000/122643808-df5bff00-d14c-11eb-9b97-fbee12369caf.PNG)

#### RSS - 회귀의 비용 함수

![rss_math](https://user-images.githubusercontent.com/70785000/122643808-df5bff00-d14c-11eb-9b97-fbee12369caf.PNG)

회귀에서 이 `RSS는 비용(Cost)`이며 `w변수(회귀계수)`로 구성되는 RSS를 <span style="color:red">비용함수</span>라고 합니다. 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 맘수가 반환하는 값(즉, 오류 값)을 지속적으로 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다. 비용함수를 손실함수(loss function)라고도 합니다.



#### 비용 최소화하기  - 경사하강법(Gradient Descent)

W파라미터의 갯수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 w 변숫값을 도출할 수 있겠지만, w파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵습니다.

경사하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식입니다.

![gradient_descent](https://user-images.githubusercontent.com/70785000/122644851-3f08d900-d152-11eb-8a2a-e59e876694f0.PNG)

경사하강법의 사전적 의미인 `점진적인 하강`이라는 뜻에서도 알수 있듯이, `점진적으로 반복적인 계산을 통해서 W파라미터 값을 업데이트하면서 오류값이 최소가 되는 W파라미터를 구하는 방식`.

* 경사하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W파라미터를 지속해서 보정해 나갑니다.
* 최초 오류 값이 `100`이었다면 두 번째 오류값은`100`보다 작은 `90`, 세 번째는 `80`과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 `W값`을 계속 업데이트 해 나갑니다.
* 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 `W값`을 최적 파라미터로 반환합니다

#### 미분을 통해 비용 함수의 최소값을 찾기

어떻게 하면 오류가 작아지는 방향으로 W값을 보정할 수 있을까?

​                                            <span style="color:red">미분은 증가 또는 감소의 방향성을 의미</span>

![gradient_descent](https://user-images.githubusercontent.com/70785000/122644851-3f08d900-d152-11eb-8a2a-e59e876694f0.PNG)

비용함수가 다음 같은 포물선 형태의 2차 함수라면 경사하강법은 최조 w에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 w를 업데이트합니다.

마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 w를 반환합니다.

#### RSS의 편미분

R(w)는 변수가 w 파라미터로 이뤄진 함수이며, 
$$
R(w) = \frac {1}{N}\sum_{i=1}^N (y_i - (w_0 + w_1 * x_i))^2
$$
입니다. R(w)를 미분해 미분함수의 최솟값을 구해야 하는데, R(w)는 두개의 w파라미터인 w0와 w1을 각각 가지고 있기 때문에 일반적인 미분을 적용할 수가 없고, w0, w1 각 변수에 편미분을 적용해야 합니ㅏㄷ.

R(w)를 최소화하는 w0와 w1 값은 각각 r(w)를 w0, w1으로 순차적으로 편미분을 수행해 얻을 수 있습니다.

![rss_편미분](https://user-images.githubusercontent.com/70785000/122647666-5353d280-d160-11eb-8ced-749a0470ff98.PNG)

#### RSS의 편미분 - W1으로 편미분



#### RSS의 편미분 - W0으로 편미분

![편미분풀어헤치기_1](https://user-images.githubusercontent.com/70785000/122697075-82ad3100-d27f-11eb-8782-c2adf1dbb2d7.PNG)

#### 식을 풀어 헤치면

![편미분풀어헤치기](https://user-images.githubusercontent.com/70785000/122697115-9d7fa580-d27f-11eb-8ee7-f0e408d9e9ed.PNG)

#### 위와 같이 w1, w0의 편미분 결과값인 최종 결과값을 반복적으로 보정하면서 w1, w0값을 업데이트 하면 비용함수 r(w)가 최소가 되는 w1, w0값을 구할 수 있습니다. 

하지만 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정 계수 &eta;를 곱하는데 , 이를 `학습률`이라고 한다.

**경사하강법은 아래와 같은 새로운 w1, 새로운 w0를 반복적으로 업데이트 하면서 비용 함수가 최소가 되는 값을 찾습니다.**

![최종공식](https://user-images.githubusercontent.com/70785000/122697190-c6079f80-d27f-11eb-8d8d-0cc722596e71.PNG)

#### 경사하강법 수행 프로세스

* `Step1` - `w1, w0`를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
* `Step2` -` w1과 w0`의 값을 위의 비용함수의 계산식으로 업데이트 한 후 다시 비용 함수의 값을 계산
* `Step3` - 비용 함수의 값이 감소했으면 다시 `Step2`를 반복합니다.  더 이상의 비용 함수의 값이 감소하지 않으면 그때의 `w1, w0`를 구하고 반복을 중지합니다.

