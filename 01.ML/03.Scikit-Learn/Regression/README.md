### 회귀 알고리즘

회귀는 현대 통계학을 이루는 큰 축이며 유전적 특성을 연구한 영국의 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다는 것이 일반적이다.

데이터분석가들은 분류보다는 회귀를 지도 학습 분야에서 가장 많이 활용한다.

"부모의 키가 크더라도 자식의 키가 대를 이어 무한정  커지지 않으며 , 부모의 키가 작더라도 대를 이어 자식의 키가 무한정 작아지지 않는다"

#### 회귀 개요

* 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭하며, 결정값이 주어지는 지도학습이라고 할 수 있다.

회귀분석은 이처럼 데이터 값이 평균과 같은 일정값으로 돌아가려는 경향을 이용한 통계학 기법

아파트 가격의 결정은 방 개수, 아파트 크기, 주변 학군, 근처 지하철 역 갯수등 다양한 독립변수로의 결합으로 이루어진다고 가정해보자.

그럼, 아래와 같은 수식을 생각해본다면,
$$
Y = W_1 * X_1 + W_2 * X_2 + W_3 * X_3 + .... + W_n * X_n
$$
`Y는 종속변수, 즉 아파트 가격`
$$
X_1, X_2, X_3, ....X_n 은 방 개수, 아파트 크기, 주변 학군등의 독립변수
$$

$$
W_1, W_2, W_3, ....W_n은 이 독립변수의 값에 영향을 미치는 회귀계수
$$

<span style="color:red"><b>`머신러닝 회귀 예측`의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것</b></span>

#### 회귀의 유형

* 회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러가지 유형으로 나눌 수 있습니다. 회귀에서 가장 중요한 것은 바로 회귀계수(`coefficient`)입니다. 이 회귀계수가 `선형이냐 비선형`에 따라 선형 회귀와 비선형 회귀로 나눌 수 있습니다. 그리고 독립 변수의 갯수가 한 개인지 여러 개인지에 따라 단일회귀, 다중 회귀로 나뉩니다.

  | 독립변수 갯수      | 회귀 계수의 결합    |
  | ------------------ | ------------------- |
  | 1개 : 단일 회귀    | 선형 : 선형 회귀    |
  | 여러 개: 다중 회귀 | 비선형: 비선형 회귀 |


#### 분류와 회귀

| 분류(Classification)                                         | 회귀(Regression)                             |
| ------------------------------------------------------------ | -------------------------------------------- |
| 결과값 : category값(이산값)으로 0, 1, 2,3과 같이 불연속적인 값<br />예) 등급, 고양이 혹은 개 판정 | 숫자값으로 연속적인 값<br />예) 비율, 점유울 |

#### 선형회귀의 종류

* 일반 선형 회귀 : 예측값과 실제 값의 `RSS(Residual Sum of Squares)`를 최소화 할 수 있도록 회귀 계수를 최적화하며, 규제(Regulation)를 적용하지 않은 모델

* `릿지(Ridge)`: 릿지 회귀는 선형 회귀에 `L2규제`를 추가한 회귀 모델

* `라쏘(Lasso)` : 라쏘 회귀는 선형 회귀에 `L1규제`를 적용한 방식

* `엘라스틱넷(ElasticNet)` : `L2, L1규제`를 함께 결합한 모델

* `로지스틱 회귀(Logistic Regression)` : 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형모델

  예) `0, 1`의 값을 예측(이산) - 바이너리 분류에 효과적

  

#### 단순 선형 회귀(Simple Regression)를 통한 회귀의 이해

주택 가격이 단순히 주택의 크기로만 결정되는 단순 선형 회귀로 가정하면 다음과 같이 주택가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현 할 수 있다.

회귀는 우측의 이미지처럼 `실제값과 모델 사이의 오류값의 차이를 최소화 하는 것`을 목표로 하며, 점선형태의  `빨간색 ` 회귀선처럼 오류차이가 많이 나면 좋지 않다고 할 수 있다.

| <img src="https://user-images.githubusercontent.com/70785000/122632039-07c30980-d10b-11eb-950c-e0106cba710a.PNG" alt="regression_1" style="zoom:80%;" /> | <img src="https://user-images.githubusercontent.com/70785000/122632286-d0edf300-d10c-11eb-853d-5805356da673.PNG" alt="regression_2" style="zoom:80%;" /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

<span style="color:red">`최적의 회귀 모델을 만든다는 것은 바로 전체 데이터의 잔차(오류 값)합이 최소가 되는 모델을 만든다는 의미이며, 동시에 오류 값이 최소가 될 수 있는 최적의 회귀계수를 찾는다는 의미도 된다.`</span>

#### RSS기반의 회귀 오류 측정

RSS란?

오류 값의 제곱을 구해서 더하는 방식. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS방식으로 오류 합을 구합니다. 
$$
Error_2 = RSS
$$
![rss](https://user-images.githubusercontent.com/70785000/122643155-3233b780-d149-11eb-9f75-f6f70c286b62.PNG)

#### RSS의 이해

- RSS는 이제 변수가 w0,w1인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심사항

- RSS는 <span style="color:red">회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라, w변수(회귀계수)가 중심 변수 임을 인지하는 것이 매우 중요합니다.</span>(학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주합니다)

- 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.(i는 1부터 학습 데이터의 총 건수 N까지)

  ![rss_math](https://user-images.githubusercontent.com/70785000/122643808-df5bff00-d14c-11eb-9b97-fbee12369caf.PNG)

#### RSS - 회귀의 비용 함수

![rss_math](https://user-images.githubusercontent.com/70785000/122643808-df5bff00-d14c-11eb-9b97-fbee12369caf.PNG)

회귀에서 이 `RSS는 비용(Cost)`이며 `w변수(회귀계수)`로 구성되는 RSS를 <span style="color:red">비용함수</span>라고 합니다. 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 맘수가 반환하는 값(즉, 오류 값)을 지속적으로 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다. 비용함수를 손실함수(loss function)라고도 합니다.



#### 비용 최소화하기  - 경사하강법(Gradient Descent)

W파라미터의 갯수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 w 변숫값을 도출할 수 있겠지만, w파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵습니다.

경사하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식입니다.

![gradient_descent](https://user-images.githubusercontent.com/70785000/122644851-3f08d900-d152-11eb-8a2a-e59e876694f0.PNG)

경사하강법의 사전적 의미인 `점진적인 하강`이라는 뜻에서도 알수 있듯이, `점진적으로 반복적인 계산을 통해서 W파라미터 값을 업데이트하면서 오류값이 최소가 되는 W파라미터를 구하는 방식`.

* 경사하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W파라미터를 지속해서 보정해 나갑니다.
* 최초 오류 값이 `100`이었다면 두 번째 오류값은`100`보다 작은 `90`, 세 번째는 `80`과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 `W값`을 계속 업데이트 해 나갑니다.
* 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 `W값`을 최적 파라미터로 반환합니다

#### 미분을 통해 비용 함수의 최소값을 찾기

어떻게 하면 오류가 작아지는 방향으로 W값을 보정할 수 있을까?

​                                            <span style="color:red">미분은 증가 또는 감소의 방향성을 의미</span>

![gradient_descent](https://user-images.githubusercontent.com/70785000/122644851-3f08d900-d152-11eb-8a2a-e59e876694f0.PNG)

비용함수가 다음 같은 포물선 형태의 2차 함수라면 경사하강법은 최조 w에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 w를 업데이트합니다.

마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 w를 반환합니다.

#### RSS의 편미분

R(w)는 변수가 w 파라미터로 이뤄진 함수이며, 
$$
R(w) = \frac {1}{N}\sum_{i=1}^N (y_i - (w_0 + w_1 * x_i))^2
$$
입니다. R(w)를 미분해 미분함수의 최솟값을 구해야 하는데, R(w)는 두개의 w파라미터인 w0와 w1을 각각 가지고 있기 때문에 일반적인 미분을 적용할 수가 없고, w0, w1 각 변수에 편미분을 적용해야 합니ㅏㄷ.

R(w)를 최소화하는 w0와 w1 값은 각각 r(w)를 w0, w1으로 순차적으로 편미분을 수행해 얻을 수 있습니다.

![rss_편미분](https://user-images.githubusercontent.com/70785000/122647666-5353d280-d160-11eb-8ced-749a0470ff98.PNG)

###### RSS의 편미분 - W1으로 편미분



###### RSS의 편미분 - W0으로 편미분

![편미분풀어헤치기_1](https://user-images.githubusercontent.com/70785000/122697075-82ad3100-d27f-11eb-8782-c2adf1dbb2d7.PNG)

식을 풀어헤치면

![편미분풀어헤치기](https://user-images.githubusercontent.com/70785000/122697115-9d7fa580-d27f-11eb-8ee7-f0e408d9e9ed.PNG)

위와 같이 w1, w0의 편미분 결과값인 최종 결과값을 반복적으로 보정하면서 w1, w0값을 업데이트 하면 비용함수 r(w)가 최소가 되는 w1, w0값을 구할 수 있습니다. 

하지만 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정 계수 &eta;를 곱하는데 , 이를 `학습률`이라고 한다.

**경사하강법은 아래와 같은 새로운 w1, 새로운 w0를 반복적으로 업데이트 하면서 비용 함수가 최소가 되는 값을 찾습니다.**

![최종공식](https://user-images.githubusercontent.com/70785000/122697190-c6079f80-d27f-11eb-8d8d-0cc722596e71.PNG)

#### 경사하강법 수행 프로세스

* `Step1` - `w1, w0`를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
* `Step2` -` w1과 w0`의 값을 위의 비용함수의 계산식으로 업데이트 한 후 다시 비용 함수의 값을 계산
* `Step3` - 비용 함수의 값이 감소했으면 다시 `Step2`를 반복합니다.  더 이상의 비용 함수의 값이 감소하지 않으면 그때의 `w1, w0`를 구하고 반복을 중지합니다.

#### 사이킷런 `LinearRegression`클래스

##### LinearRegression클래스

`class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X = True, n_jobs=1)`

* `linearRegressoin`클래스는 예측값과 실제 값의 `RSS(Residual Sum of Squares)`를 최소화해 `OLS(Ordinary Least Squares)추정 방식`으로 구현한 클래스

* `LinearRegression`클래스는 `fit()`메소드로 X, y 배열을 입력받으면 회귀 계수(`coefficients)인  W`를 `coef_`속성에 저장

  | 입력 파라미터 | `fit_intercept` : 불린 값으로 디폴트는 True<br />`Intercept(절편)`값을 계산할 것인지 말지를 지정<br />만일 False로 지정하면 `intercept`가 사용되지 않고 `0`으로 지정된다.<br /><br />normalize : 불린값으로 디폴트는 False.`fit_intercept`파라미터가 False인 경우는 이 파라미터가 무시됩니다. 만일 True이면 회귀를 수행하기 전에 입력 데이터세트를 정규화한다.<br />![fit_intercept_true](https://user-images.githubusercontent.com/70785000/122735569-1e5b9300-d2ba-11eb-948e-bfb74c977c6d.PNG) |
  | ------------- | ------------------------------------------------------------ |
  | 속성          | coef_`: fit()`메소드를 수행했을 때 회귀 계수가 배열 형태로 저장하는 속성.<br />`Shape는 (Target값 개수, 피처갯수)`<br />`intercept_ : intercept값` |

##### 선형회귀의 다중 공선성 문제

* 일반적으로 선형 회귀는 입력 피처의 독립성에 많은 영향을 받습니다. 피처간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해집니다. 이러한 현상을 다중 공선성(`multi-collinearity`)문제하고 합니다. 일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용합니다.

  예) `A`라는 피처가 방의 크기인데 평이고, 어떤 피처는 제곱미터(m^2)가 되어있는 예

##### 회귀평가지표

| 평가지표 | 설명                                                         | 수식                                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `MAE`    | `Mean Absolute Error(MAE)`이며 실제 값과 예측값의 차이를 절대값으로 변환해 평균한 것. | ![MAE](https://user-images.githubusercontent.com/70785000/122759351-cc743680-d2d4-11eb-895c-9c069dc28127.PNG) |
| `MSE`    | `Mean Squared Error(MSE)`이며 실제 값과 예측값의 차이를 제곱해 평균한 것 | ![MSE](https://user-images.githubusercontent.com/70785000/122759669-2bd24680-d2d5-11eb-8aa5-486592971274.PNG) |
| `MSLE`   | `MSE`에 로그를 적용한 것. 결정값이 클수록 오류값도 커지기 때문에 일부 큰 오류값들로 인해 전체 오류값이 커지는 것을 막아줍니다.<br />예) 매출금액과 같이 금액이 큰 경우. | `Log(MSE)`                                                   |
| `RMSE`   | `MSE`값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있으므로 `MSE`에 루트를 씌운 것이 `RMSE(Root Mean Squared Error)`입니다. | ![RMSE](https://user-images.githubusercontent.com/70785000/122760009-984d4580-d2d5-11eb-8924-7a531521ac7a.PNG) |
| `RMSLE`  | `RMSE`에 로그를 적용한 것입니다. 결정값이 클수록 오류값도 커지기 때문에 일부 큰 오류값들로 인해 전체 오류값이 커지는 것을 막아줍니다. | `Log(RMSE)`                                                  |
| `R^2`    | 분산 기반으로 예측 성능을 평가합니다.실제 값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높습니다. | ![R2](https://user-images.githubusercontent.com/70785000/122760221-d9ddf080-d2d5-11eb-877f-1d1cbf091c93.PNG) |

##### 사이킷런 회귀 평가 API

* 사이킷런은 아쉽게도 RMSE를 제공하지 않습니다. RMSE를 구하기 위해서는 MSE에 제곱근을 씌우서 계산하는 함수를 직접 만들어야 합니다.
* 다음은 각 평가 방법에 사이킷런의 API및 `cross_val_score`나 `GridSearchCV`에서 평가 시 사용되는 `scoring파라미터의 적용 값`입니다.

| 평가방법 | 사이킷런 평가지표 API         | Scoring 함수 적용 값        |
| -------- | ----------------------------- | --------------------------- |
| MAE      | `metrics.mean_absolute_error` | `'neg_mean_absolute_error'` |
| MSE      | `metrics.mean_squared_error`  | `'neg_mean_squared_error'`  |
| R^2      | `metrics.r2_score`            | `'r^2'`                     |

###### 사이킷런 Scoring함수에 회귀 평가 적용시 유의 사항

**`cross_val_score, GridSearchCV와 같은 Scoring함수에 평가지표를 적용시 유의사항`**

* `MAE`의 사이킷런 scoring파라미터 값은 `'neg_mean_absolute_error'`입니다. 이는 `Negative(음수)값`을 가진다는 의미인데, MAE는 절댓값의 합이기 때문에 음수가 될 수 없습니다.
* Scoring함수에 `'neg_mean_absolute_error'`를 적용해 음수값을 반환하는 이유는 사이킷런의 `Scoring함수가 score값`이 클수록 좋은  평가결과로 자동 평가하기 때문입니다. 따라서 `-1`을 원래의 평가지표 값에 곱해서 음수(`negative`)를 만들어 작은 오류 값이 더 큰 숫자로 인식하게 됩니다. 예를 들어 `10 > 1`이지만 음수를 곱하면 `-1 > -10`이 됩니다.(오류값이 작을 수록 좋은 지표이기 때문에 원래의 평가지표에 `-1`을 곱함)
* `metrics.mean_absolute_error()`와 같은 사이킷런 평가 지표 API는 정상적으로 양수의 값을 반환합니다. 하지만 `Scoring함수`의 `scoring파라미터 값` `'neg_mean_absolute_error'`가 의미하는 것은 `-1 * metrics.mean_absolute_error()`이니 주의가 필요합니다.
