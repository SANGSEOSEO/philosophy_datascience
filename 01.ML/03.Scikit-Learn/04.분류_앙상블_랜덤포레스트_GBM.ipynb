{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류(Classification)\n",
    "## 4.3 앙상블 \n",
    "* 앙상블 학습법은 여러개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법\n",
    "* 협업, 오케스트라연주로 이해하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블 학습의 유형\n",
    "   * Voting방식 : 서로 다른 유형의 분류기가 예측을 수행후 그 예측값들 중 가장 많이 나온것을 채택(다수결)\n",
    "   * Bagging방식 : 각각의 분류기가 서로 동일한 유형의 알고리즘이지만, 데이터샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅하는데 \n",
    "     채택하는 방식은 소프트보팅(각각 분류기의 예측값들의 평균값), 하드보팅(다수결의 원칙)\n",
    "   * Boosting방식\n",
    "     - 그래디언부스팅, XGBoost, Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보팅분류기(Voting Classifier)\n",
    "* 사이킷런에서 제공하는 보팅 방식의 앙상블을 구현한 Voting Classifier클래스를 통해 로지스틱회귀와 KNN을 기반으로 보팅분류기 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**라이브러리 로드 및 유방암데이터 세트 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38           122.8     1001.0          0.11840   \n",
       "1        20.57         17.77           132.9     1326.0          0.08474   \n",
       "2        19.69         21.25           130.0     1203.0          0.10960   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33            184.6   \n",
       "1                 0.05667  ...         24.99          23.41            158.8   \n",
       "2                 0.05999  ...         23.57          25.53            152.5   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data = cancer.data, columns = cancer.feature_names)\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소프트 보팅방식으로 로직스틱회귀와 KNN기반으로 보팅 분류기 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting분류기 정확도 : 0.9474\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "# 개별 모델을 소프트 보팅 기반으리 양상ㅂㄹ 모델로 구현한 분류기\n",
    "# estimator에 인자 던질때 help문서 참조\n",
    "# 안 그러면 에러가 이렇게 나네. -- TypeError: 'LogisticRegression' object is not iterable\n",
    "vo_clf = VotingClassifier(estimators = [('LR', lr_clf), ('KNN', knn_clf)], voting = 'soft')  # 디폴트값은 hard\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 156)\n",
    "\n",
    "# VotingClassifier로 학습/예측/평가 수행\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on VotingClassifier in module sklearn.ensemble._voting object:\n",
      "\n",
      "class VotingClassifier(sklearn.base.ClassifierMixin, _BaseVoting)\n",
      " |  VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      " |  \n",
      " |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimators : list of (str, estimator) tuples\n",
      " |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      " |      of those original estimators that will be stored in the class attribute\n",
      " |      ``self.estimators_``. An estimator can be set to ``'drop'``\n",
      " |      using ``set_params``.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |          ``'drop'`` is accepted.\n",
      " |  \n",
      " |      .. deprecated:: 0.22\n",
      " |         Using ``None`` to drop an estimator is deprecated in 0.22 and\n",
      " |         support will be dropped in 0.24. Use the string ``'drop'`` instead.\n",
      " |  \n",
      " |  voting : {'hard', 'soft'}, default='hard'\n",
      " |      If 'hard', uses predicted class labels for majority rule voting.\n",
      " |      Else if 'soft', predicts the class label based on the argmax of\n",
      " |      the sums of the predicted probabilities, which is recommended for\n",
      " |      an ensemble of well-calibrated classifiers.\n",
      " |  \n",
      " |  weights : array-like of shape (n_classifiers,), default=None\n",
      " |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      " |      predicted class labels (`hard` voting) or class probabilities\n",
      " |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel for ``fit``.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  flatten_transform : bool, default=True\n",
      " |      Affects shape of transform output only when voting='soft'\n",
      " |      If voting='soft' and flatten_transform=True, transform method returns\n",
      " |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      " |      flatten_transform=False, it returns\n",
      " |      (n_classifiers, n_samples, n_classes).\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      If True, the time elapsed while fitting will be printed as it\n",
      " |      is completed.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      " |      that are not 'drop'.\n",
      " |  \n",
      " |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Attribute to access any fitted sub-estimators by name.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  classes_ : array-like of shape (n_predictions,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  VotingRegressor: Prediction voting regressor.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      " |  >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
      " |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
      " |  >>> clf3 = GaussianNB()\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> eclf1 = VotingClassifier(estimators=[\n",
      " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      " |  >>> eclf1 = eclf1.fit(X, y)\n",
      " |  >>> print(eclf1.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
      " |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
      " |  True\n",
      " |  >>> eclf2 = VotingClassifier(estimators=[\n",
      " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      " |  ...         voting='soft')\n",
      " |  >>> eclf2 = eclf2.fit(X, y)\n",
      " |  >>> print(eclf2.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  >>> eclf3 = VotingClassifier(estimators=[\n",
      " |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      " |  ...        voting='soft', weights=[2,1,1],\n",
      " |  ...        flatten_transform=True)\n",
      " |  >>> eclf3 = eclf3.fit(X, y)\n",
      " |  >>> print(eclf3.predict(X))\n",
      " |  [1 1 1 2 2 2]\n",
      " |  >>> print(eclf3.transform(X).shape)\n",
      " |  (6, 6)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VotingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      _BaseVoting\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the estimators.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if all underlying estimators\n",
      " |          support sample weights.\n",
      " |      \n",
      " |          .. versionadded:: 0.18\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      maj : array-like of shape (n_samples,)\n",
      " |          Predicted class labels.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Return class labels or probabilities for X for each estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      probabilities_or_labels\n",
      " |          If `voting='soft'` and `flatten_transform=True`:\n",
      " |              returns ndarray of shape (n_classifiers, n_samples *\n",
      " |              n_classes), being class probabilities calculated by each\n",
      " |              classifier.\n",
      " |          If `voting='soft' and `flatten_transform=False`:\n",
      " |              ndarray of shape (n_classifiers, n_samples, n_classes)\n",
      " |          If `voting='hard'`:\n",
      " |              ndarray of shape (n_samples, n_classifiers), being\n",
      " |              class labels predicted by each classifier.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      avg : array-like of shape (n_samples, n_classes)\n",
      " |          Weighted average probability for each class per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseVoting:\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          Setting it to True gets the various classifiers and the parameters\n",
      " |          of the classifiers as well.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with `get_params()`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : keyword arguments\n",
      " |          Specific parameters using e.g.\n",
      " |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      " |          parameters of the stacking estimator, the individual estimator of\n",
      " |          the stacking estimators can also be set, or can be removed by\n",
      " |          setting them to 'drop'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  named_estimators\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      " |  \n",
      " |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vo_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**개별 모델의 학습/예측/평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting분류기 정확도 : 0.9386\n",
      "LogisticRegression 정확도 : 0.9386\n",
      "KNeighborsClassifier 정확도 : 0.9386\n"
     ]
    }
   ],
   "source": [
    "print(\"Voting분류기 정확도 : {0:.4f}\".format(accuracy_score(y_test, pred)))\n",
    "classifiers = [lr_clf, knn_clf]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name = classifier.__class__.__name__\n",
    "    print(\"{0} 정확도 : {1:.4f}\".format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 렌덤포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤포레스트의 개요와 실습\n",
    "* 같은 알고리즘으로 여러개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘.(현실 세계의 정치에서 행하는 행위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도 : 0.9253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import load_datasets\n",
    "\n",
    "# 결정트리에서 사용한 get_human_datasets()를 이용해 학습/데이터세트 데이터 프레임 변환\n",
    "X_train, X_test, y_train, y_test = load_datasets.get_human_dataset()\n",
    "\n",
    "# 랜덤 포레스트 하습 및 별도의 테스트 세트로 예측/성능 /평가\n",
    "rf_clf = RandomForestClassifier(random_state = 0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(\"랜덤 포레스트 정확도 : {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GridSearchCV를 이용해 랜덤포레스트의 하이퍼 파라미터 튜닝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터 : \n",
      " {'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "최고 예측 정확도 : \n",
      " 0.9180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators': [100],\n",
    "         'max_depth': [6, 8, 10, 12],\n",
    "         'min_samples_leaf': [8, 12, 18],\n",
    "         'min_samples_split': [8, 16, 20]\n",
    "         }\n",
    "\n",
    "# RandomForestClassifier객체 생성 후 GridSearchCV수행\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_jobs = 1)\n",
    "gridCV = GridSearchCV(estimator = rf_clf, param_grid = params, cv = 2, n_jobs = 1)\n",
    "gridCV.fit(X_train, y_train)\n",
    "\n",
    "print(\"최적 하이퍼 파라미터 : \\n\", gridCV.best_params_)\n",
    "print(\"최고 예측 정확도 : \\n {0:.4f}\".format(gridCV.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_estimators = 300으로 재수행**\n",
    "* 나머지 파라미터는 위의 파라미터로 재수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : \n",
      " 0.9165\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 300, max_depth = 10, min_samples_leaf = 8, min_samples_split = 8, random_state = 0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "print(\"예측 정확도 : \\n {0:.4f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**피처 중요도에 대한 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ftr_importance_values = rf_clf.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importance_values, index = X_train.columns)\n",
    "\n",
    "ftr_top20 = ftr_importances.sort_values(ascending = False)[:20]\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.title(\"Feature Top 20 importances\")\n",
    "sns.barplot(x = ftr_top20, y = ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tGravityAcc-min()-X, tGravityAcc-mean()-Y, tGrativityAcc-min()-Y등이 높은 피처 중요도를 갖음**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 GBM(Gradient Boosting Machine)\n",
    "* GBM은 약한 학습기를 순차적으로 학습시키면면서 예측시에 잘못 분류된 데이터에 대해서 가중치를 부여를 통해 오류를 개선시키는 방식\n",
    "* 가장 대표적인 구현은 Ada Boost(Adaptive Boosting)방식이며,이와 비슷하지만 오류값을 순차 학습시키면서 최소화하는 방식으로 진행하는 경사하강법(Gradient Decent)방식이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM을 이용해 사용자 행동 데이터세트 예측 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import load_datasets\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_datasets.get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM정확도 : 0.9389\n",
      "GBM수행시간 : 2617.9\n"
     ]
    }
   ],
   "source": [
    "# GBM 수행시간 측정을 위함 시작시간 설정\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(random_state = 0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(\"GBM정확도 : {0:.4f}\".format(gb_accuracy))\n",
    "print(\"GBM수행시간 : {0:.1f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**본인 PC환경 : 총걸린시간: 2617초.ㅠㅠ ( 메모리 4GB, CPU코어 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      " |      loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are 'friedman_mse' for the mean squared error with improvement\n",
      " |      score by Friedman, 'mse' for mean squared error, and 'mae' for\n",
      " |      the mean absolute error. The default value of 'friedman_mse' is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random spliting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  presort : deprecated, default='deprecated'\n",
      " |      This parameter is deprecated and will be removed in v0.24.\n",
      " |  \n",
      " |      .. deprecated :: 0.22\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_classification(random_state=0)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      " |  ...     X, y, random_state=0)\n",
      " |  >>> clf = GradientBoostingClassifier(random_state=0)\n",
      " |  >>> clf.fit(X_train, y_train)\n",
      " |  GradientBoostingClassifier(random_state=0)\n",
      " |  >>> clf.predict(X_test[:2])\n",
      " |  array([1, 0])\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.88\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.ensemble.HistGradientBoostingClassifier,\n",
      " |  sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n",
      " |  AdaBoostClassifier\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape [n_samples].\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM 하이퍼 파라미터 및 튜닝\n",
    "* loss : GD(Gradient Decent)에서 사용할 비용함수를 지정.특별한 이유가 없는한 기본값이 `deviance`를 그대로 적용\n",
    "* learning_rate : GBM이 학습을 진행할때마다 적용하는 학습률.(오류값을 보정해 나가는데 적용하는 계수.0과 1사이의 값이며 기본값은 0.1)\n",
    "* `n_estimators` : 약한 학습기의 갯수를 의미.weak learner가 순차적으로 오류를 보정하므로 개수가 많을수로고 예측성능이 일정 수준까지는 좋아질수있습닏. 하지만 갯수가 많을수록 수행 시간이 오래 걸립니다. 기본값 100\n",
    "* `subsample` : weak learner가 학습에 사용하는 데이터샘플링 비율: 기본값은 1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV를 이용한 하이터 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-fa43e00dfd7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      7\u001b[0m \u001b[0mgrid_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"최적 하이터 파라미터: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"최고 예측 정확도: {0:.4f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 500],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "grid_cv = GridSearchCV(gb_clf, param_grid = params, cv =2, verbose = 1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(\"최적 하이터 파라미터: \\n\", grid_cv.best_params_)\n",
    "print(\"최고 예측 정확도: {0:.4f}\".format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위에서 도출된 최적의 파라미터를 그대로 적용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "gb_accuracy = accuracy_sore(y_test, gb_pred)\n",
    "print(\"GBM정확도 : {0:.4f}\".format(gb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GBM은 과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘이다. 하지만 수행시간이 오래 걸린다는 단점이 있다.\n",
    "* 앙상브리법은 대표적으로 배깅과 부스팅으로 구분될 수 있으며, 배깅방식은 학습 데이터를 중복허용하면서 다수이 세트로 샘플링하여 이를 다수의 약한 학습기가 학습한 뒤 최종 결과를 결합해 예측하는 방식\n",
    "* 대표적인 배깅방식이 랜덤 포레스트로서, 랜덤포레스트는 수행시간이 빠르고 비교적 안정적인 예측 성능을 제공하는 훌률한 머신러닝 알고리즘."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
